//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-26907403
// Cuda compilation tools, release 10.1, V10.1.243
// Based on LLVM 3.4svn
//

.version 6.4
.target sm_70
.address_size 64

.func _ZN2S23incEv
(
	.param .b64 _ZN2S23incEv_param_0
)
;
.func _ZN2S23decEv
(
	.param .b64 _ZN2S23decEv_param_0
)
;
.global .attribute(.managed) .align 16 .b8 buf5[128];
.global .attribute(.managed) .align 8 .u64 range_tree;
.global .attribute(.managed) .align 4 .u32 tree_size;
.global .attribute(.managed) .align 8 .u64 temp_coal;
.global .align 8 .u64 _ZTV2S2[4] = {0, 0, _ZN2S23incEv, _ZN2S23decEv};

.func _ZN2S23incEv(
	.param .b64 _ZN2S23incEv_param_0
)
{
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<2>;


	ld.param.u64 	%rd1, [_ZN2S23incEv_param_0];
	ld.u32 	%r1, [%rd1+8];
	add.s32 	%r2, %r1, 2;
	st.u32 	[%rd1+8], %r2;
	ret;
}

.func _ZN2S23decEv(
	.param .b64 _ZN2S23decEv_param_0
)
{
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<2>;


	ld.param.u64 	%rd1, [_ZN2S23decEv_param_0];
	ld.u32 	%r1, [%rd1+8];
	add.s32 	%r2, %r1, -2;
	st.u32 	[%rd1+8], %r2;
	ret;
}

	// .globl	_Z9vptrPatchPvS_ji
.visible .entry _Z9vptrPatchPvS_ji(
	.param .u64 _Z9vptrPatchPvS_ji_param_0,
	.param .u64 _Z9vptrPatchPvS_ji_param_1,
	.param .u32 _Z9vptrPatchPvS_ji_param_2,
	.param .u32 _Z9vptrPatchPvS_ji_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<7>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [_Z9vptrPatchPvS_ji_param_0];
	ld.param.u64 	%rd2, [_Z9vptrPatchPvS_ji_param_1];
	ld.param.u32 	%r2, [_Z9vptrPatchPvS_ji_param_2];
	ld.param.u32 	%r3, [_Z9vptrPatchPvS_ji_param_3];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB2_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.lo.s32 	%r7, %r1, %r2;
	cvt.u64.u32	%rd4, %r7;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd2;
	ld.global.u8 	%rs1, [%rd6];
	ld.global.u8 	%r8, [%rd6+3];
	ld.global.u8 	%r9, [%rd6+4];
	prmt.b32 	%r10, %r9, %r8, 30212;
	ld.global.u8 	%r11, [%rd6+1];
	prmt.b32 	%r12, %r10, %r11, 4180;
	ld.global.u8 	%rs2, [%rd6+5];
	cvt.u32.u16	%r13, %rs2;
	ld.global.u8 	%rs3, [%rd6+6];
	cvt.u32.u16	%r14, %rs3;
	prmt.b32 	%r15, %r14, %r13, 30212;
	cvt.u16.u32	%rs4, %r15;
	ld.global.u8 	%rs5, [%rd6+7];
	ld.global.u8 	%r16, [%rd6+2];
	st.global.u8 	[%rd5], %rs1;
	st.global.u8 	[%rd5+7], %rs5;
	st.global.u8 	[%rd5+4], %r9;
	st.global.u8 	[%rd5+2], %r16;
	st.global.u8 	[%rd5+1], %r11;
	shr.u32 	%r17, %r12, 16;
	st.global.u8 	[%rd5+3], %r17;
	shr.u16 	%rs6, %rs4, 8;
	st.global.u8 	[%rd5+6], %rs6;
	st.global.u8 	[%rd5+5], %rs4;

BB2_2:
	ret;
}

	// .globl	_Z6kernelPP2S1
.visible .entry _Z6kernelPP2S1(
	.param .u64 _Z6kernelPP2S1_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<5>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd8, [_Z6kernelPP2S1_param_0];
	mov.u32 	%r8, %tid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %ntid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r8;
	setp.gt.s32	%p1, %r1, 511;
	@%p1 bra 	BB3_7;

	cvta.to.global.u64 	%rd10, %rd8;
	mul.wide.s32 	%rd11, %r1, 8;
	add.s64 	%rd1, %rd10, %rd11;
	ld.global.u64 	%rd2, [%rd1];
	ld.global.u64 	%rd3, [range_tree];
	ld.global.u32 	%r2, [tree_size];
	mov.u64 	%rd22, 0;
	mov.u32 	%r16, 0;
	mov.u32 	%r15, 1;
	setp.lt.u32	%p2, %r2, 2;
	@%p2 bra 	BB3_6;

BB3_2:
	cvt.u64.u32	%rd4, %r15;
	mul.wide.u32 	%rd12, %r15, 40;
	add.s64 	%rd5, %rd3, %rd12;
	ld.u64 	%rd13, [%rd5];
	setp.gt.u64	%p3, %rd13, %rd2;
	mov.u16 	%rs4, 0;
	@%p3 bra 	BB3_4;

	ld.u64 	%rd14, [%rd5+8];
	setp.ge.u64	%p4, %rd14, %rd2;
	selp.u16	%rs4, 1, 0, %p4;

BB3_4:
	add.s32 	%r13, %r16, 2;
	setp.eq.s16	%p5, %rs4, 0;
	cvt.u32.u64	%r14, %rd4;
	selp.b32	%r5, %r13, %r14, %p5;
	shl.b32 	%r16, %r5, 1;
	add.s32 	%r15, %r16, 1;
	setp.lt.u32	%p6, %r15, %r2;
	@%p6 bra 	BB3_2;

	cvt.u64.u32	%rd22, %r5;

BB3_6:
	mul.lo.s64 	%rd15, %rd22, 40;
	add.s64 	%rd16, %rd3, %rd15;
	ld.u64 	%rd17, [%rd16+24];
	ld.u64 	%rd18, [%rd17+16];
	ld.global.u64 	%rd19, [%rd1];


	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd19;
	prototype_0 : .callprototype ()_ (.param .b64 _) ;
	call 
%rd18,
	(
	param0
	)
	, prototype_0;
	
	//{
	}// Callseq End 0

BB3_7:
	ret;
}

	// .globl	_Z11dump_vtableI2S2EvPPvS2_
.visible .entry _Z11dump_vtableI2S2EvPPvS2_(
	.param .u64 _Z11dump_vtableI2S2EvPPvS2__param_0,
	.param .u64 _Z11dump_vtableI2S2EvPPvS2__param_1
)
{
	.reg .pred 	%p<13>;
	.reg .b16 	%rs<13>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<59>;


	ld.param.u64 	%rd17, [_Z11dump_vtableI2S2EvPPvS2__param_0];
	ld.param.u64 	%rd18, [_Z11dump_vtableI2S2EvPPvS2__param_1];
	mov.u64 	%rd58, 0;
	mov.u64 	%rd20, buf5;
	cvta.global.u64 	%rd21, %rd20;
	setp.eq.s64	%p1, %rd21, 0;
	@%p1 bra 	BB4_2;

	mov.u64 	%rd23, _ZTV2S2;
	add.s64 	%rd24, %rd23, 16;
	cvta.global.u64 	%rd25, %rd24;
	st.global.u64 	[buf5], %rd25;
	mov.u64 	%rd58, %rd20;

BB4_2:
	cvta.to.global.u64 	%rd2, %rd17;
	ld.global.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd58];
	mov.u32 	%r14, 0;
	shr.u16 	%rs6, %rs1, 8;
	shr.u16 	%rs8, %rs2, 8;
	shr.u16 	%rs10, %rs3, 8;
	shr.u16 	%rs12, %rs4, 8;
	cvta.to.global.u64 	%rd26, %rd18;
	st.global.u8 	[%rd26], %rs1;
	st.global.u8 	[%rd26+1], %rs6;
	st.global.u8 	[%rd26+2], %rs2;
	st.global.u8 	[%rd26+3], %rs8;
	st.global.u8 	[%rd26+4], %rs3;
	st.global.u8 	[%rd26+5], %rs10;
	st.global.u8 	[%rd26+6], %rs4;
	st.global.u8 	[%rd26+7], %rs12;

BB4_3:
	cvt.s64.s32	%rd4, %r14;
	ld.global.u64 	%rd27, [%rd58];
	mul.wide.s32 	%rd28, %r14, 8;
	add.s64 	%rd29, %rd27, %rd28;
	ld.u64 	%rd5, [%rd29];
	setp.eq.s64	%p2, %rd5, 0;
	@%p2 bra 	BB4_14;

	add.s64 	%rd6, %rd2, %rd28;
	st.global.u64 	[%rd6], %rd5;
	add.s32 	%r4, %r14, 1;
	ld.global.u64 	%rd31, [%rd58];
	mul.wide.s32 	%rd32, %r4, 8;
	add.s64 	%rd33, %rd31, %rd32;
	ld.u64 	%rd7, [%rd33];
	setp.eq.s64	%p3, %rd7, 0;
	@%p3 bra 	BB4_14;

	st.global.u64 	[%rd6+8], %rd7;
	add.s32 	%r5, %r14, 2;
	ld.global.u64 	%rd34, [%rd58];
	mul.wide.s32 	%rd35, %r5, 8;
	add.s64 	%rd36, %rd34, %rd35;
	ld.u64 	%rd9, [%rd36];
	setp.eq.s64	%p4, %rd9, 0;
	@%p4 bra 	BB4_14;

	st.global.u64 	[%rd6+16], %rd9;
	add.s32 	%r6, %r14, 3;
	ld.global.u64 	%rd37, [%rd58];
	mul.wide.s32 	%rd38, %r6, 8;
	add.s64 	%rd39, %rd37, %rd38;
	ld.u64 	%rd10, [%rd39];
	setp.eq.s64	%p5, %rd10, 0;
	@%p5 bra 	BB4_14;

	st.global.u64 	[%rd6+24], %rd10;
	add.s32 	%r7, %r14, 4;
	ld.global.u64 	%rd40, [%rd58];
	mul.wide.s32 	%rd41, %r7, 8;
	add.s64 	%rd42, %rd40, %rd41;
	ld.u64 	%rd11, [%rd42];
	setp.eq.s64	%p6, %rd11, 0;
	@%p6 bra 	BB4_14;

	st.global.u64 	[%rd6+32], %rd11;
	add.s32 	%r8, %r14, 5;
	ld.global.u64 	%rd43, [%rd58];
	mul.wide.s32 	%rd44, %r8, 8;
	add.s64 	%rd45, %rd43, %rd44;
	ld.u64 	%rd12, [%rd45];
	setp.eq.s64	%p7, %rd12, 0;
	@%p7 bra 	BB4_14;

	st.global.u64 	[%rd6+40], %rd12;
	add.s32 	%r9, %r14, 6;
	ld.global.u64 	%rd46, [%rd58];
	mul.wide.s32 	%rd47, %r9, 8;
	add.s64 	%rd48, %rd46, %rd47;
	ld.u64 	%rd13, [%rd48];
	setp.eq.s64	%p8, %rd13, 0;
	@%p8 bra 	BB4_14;

	st.global.u64 	[%rd6+48], %rd13;
	add.s32 	%r10, %r14, 7;
	ld.global.u64 	%rd49, [%rd58];
	mul.wide.s32 	%rd50, %r10, 8;
	add.s64 	%rd51, %rd49, %rd50;
	ld.u64 	%rd14, [%rd51];
	setp.eq.s64	%p9, %rd14, 0;
	@%p9 bra 	BB4_14;

	st.global.u64 	[%rd6+56], %rd14;
	add.s32 	%r11, %r14, 8;
	ld.global.u64 	%rd52, [%rd58];
	mul.wide.s32 	%rd53, %r11, 8;
	add.s64 	%rd54, %rd52, %rd53;
	ld.u64 	%rd15, [%rd54];
	setp.eq.s64	%p10, %rd15, 0;
	@%p10 bra 	BB4_14;

	st.global.u64 	[%rd6+64], %rd15;
	add.s32 	%r12, %r14, 9;
	ld.global.u64 	%rd55, [%rd58];
	mul.wide.s32 	%rd56, %r12, 8;
	add.s64 	%rd57, %rd55, %rd56;
	ld.u64 	%rd16, [%rd57];
	setp.eq.s64	%p11, %rd16, 0;
	@%p11 bra 	BB4_14;

	cvt.u32.u64	%r13, %rd4;
	st.global.u64 	[%rd6+72], %rd16;
	add.s32 	%r14, %r13, 10;
	setp.lt.s32	%p12, %r14, 30;
	@%p12 bra 	BB4_3;

BB4_14:
	ret;
}


